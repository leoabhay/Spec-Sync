{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mtcnn\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(image, target_w=224, target_h=224):\n",
    "    '''this function crop & resize images to target size by keeping aspect ratio'''\n",
    "    if image.ndim == 2:\n",
    "        img_h, img_w = image.shape             # for Grayscale will be   img_h, img_w = img.shape\n",
    "    elif image.ndim == 3:\n",
    "        img_h, img_w, channels = image.shape   # for RGB will be   img_h, img_w, channels = img.shape\n",
    "    target_aspect_ratio = target_w/target_h\n",
    "    input_aspect_ratio = img_w/img_h\n",
    "\n",
    "    if input_aspect_ratio > target_aspect_ratio:\n",
    "        resize_w = int(input_aspect_ratio*target_h)\n",
    "        resize_h = target_h\n",
    "        img = cv2.resize(image, (resize_w , resize_h))\n",
    "        crop_left = int((resize_w - target_w)/2)  ## crop left/right equally\n",
    "        crop_right = crop_left + target_w\n",
    "        new_img = img[:, crop_left:crop_right]\n",
    "    if input_aspect_ratio < target_aspect_ratio:\n",
    "        resize_w = target_w\n",
    "        resize_h = int(target_w/input_aspect_ratio)\n",
    "        img = cv2.resize(image, (resize_w , resize_h))\n",
    "        crop_top = int((resize_h - target_h)/4)   ## crop the top by 1/4 and bottom by 3/4 -- can be changed\n",
    "        crop_bottom = crop_top + target_h\n",
    "        new_img = img[crop_top:crop_bottom, :]\n",
    "    if input_aspect_ratio == target_aspect_ratio:\n",
    "        new_img = cv2.resize(image, (target_w, target_h))\n",
    "\n",
    "    plt.imshow(new_img)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()  # creates detector  \n",
    "\n",
    "def extract_face(img, target_size=(224,224)):\n",
    "    '''this functions extract the face from different images by \n",
    "    1) finds the facial bounding box\n",
    "    2) slightly expands top & bottom boundaries to include the whole face\n",
    "    3) crop into a square shape\n",
    "    4) resize to target image size for modelling\n",
    "    5) if the facial bounding box in step 1 is not found, image will be cropped & resized to 224x224 square'''\n",
    "           \n",
    "    # 1. detect faces in an image\n",
    "      \n",
    "    results = detector.detect_faces(img)\n",
    "    if results == []:    # if face is not detected, call function to crop & resize by keeping aspect ratio\n",
    "        new_face = crop_and_resize(img, target_w=224, target_h=224)    \n",
    "    else:\n",
    "        x1, y1, width, height = results[0]['box']\n",
    "        x2, y2 = x1+width, y1+height\n",
    "        face = img[y1:y2, x1:x2]  # this is the face image from the bounding box before expanding bbox\n",
    "\n",
    "        # 2. expand the top & bottom of bounding box by 10 pixels to ensure it captures the whole face\n",
    "        adj_h = 10\n",
    "\n",
    "        #assign value of new y1\n",
    "        if y1-adj_h <10:\n",
    "            new_y1=0\n",
    "        else:\n",
    "            new_y1 = y1-adj_h\n",
    "\n",
    "        #assign value of new y2    \n",
    "        if y1+height+adj_h < img.shape[0]:\n",
    "            new_y2 = y1+height+adj_h\n",
    "        else:\n",
    "            new_y2 = img.shape[0]\n",
    "        new_height = new_y2 - new_y1\n",
    "\n",
    "        # 3. crop the image to a square image by setting the width = new_height and expand the box to new width\n",
    "        adj_w = int((new_height-width)/2)    \n",
    "\n",
    "        #assign value of new x1\n",
    "        if x1-adj_w < 0:\n",
    "            new_x1=0\n",
    "        else:\n",
    "            new_x1 = x1-adj_w\n",
    "\n",
    "        #assign value of new x2\n",
    "        if x2+adj_w > img.shape[1]:\n",
    "            new_x2 = img.shape[1]\n",
    "        else:\n",
    "            new_x2 = x2+adj_w\n",
    "        new_face = img[new_y1:new_y2, new_x1:new_x2]  # face-cropped square image based on original resolution\n",
    "\n",
    "    # 4. resize image to the target pixel size\n",
    "    sqr_img = cv2.resize(new_face, target_size)   \n",
    "    return sqr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_dict = {0: 'Heart', 1: 'Oblong', 2: 'Oval', 3: 'Round', 4: 'Square'}\n",
    "\n",
    "def predict_face_shape(img_array):\n",
    "    '''\n",
    "    this function reads a single image in the form of an array, \n",
    "    and process the image then make predictions.\n",
    "    '''\n",
    "    try:\n",
    "        # first extract the face using bounding box\n",
    "        face_img = extract_face(img_array)  # call function to extract face with bounding box\n",
    "        new_img = cv2.cvtColor(face_img,cv2.COLOR_BGR2RGB) # convert to RGB -- use this for display          \n",
    "        # convert the image for modelling\n",
    "        test_img = np.array(new_img, dtype=float)\n",
    "        test_img = test_img/255 #image normalization\n",
    "        test_img = np.array(test_img).reshape(1, 224, 224, 3)  \n",
    "        # make predictions\n",
    "        pred = model.predict(test_img)        \n",
    "        label = np.argmax(pred,axis=1)\n",
    "        shape = y_label_dict[label[0]]\n",
    "        print(f'Your face shape is {shape}')\n",
    "        pred = np.max(pred)\n",
    "        print(f'Probability {np.around(pred*100,2)}')\n",
    "        plt.imshow(new_img)\n",
    "    except Exception as e:\n",
    "        print(f'Oops!  Something went wrong.  Please try again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Transfer Learning model from VGG-Face\n",
    "\n",
    "transfer_path = \"C:\\\\Users\\\\tprak_o7ept1f\\\\OneDrive\\\\Desktop\\\\Face_Shape_Classification\\\\saved_models\\\\\"\n",
    "\n",
    "transfer_file = transfer_path + 'vgg16-face-1.h5'\n",
    "model = tf.keras.models.load_model(transfer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img =  cv2.imread( r\"C:\\Users\\abhay\\Desktop\\Abhay\\Pictures\\Pins\\IMG_1237.JPG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
