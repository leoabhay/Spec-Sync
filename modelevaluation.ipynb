{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import os \n",
    "import cv2\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['Heart','Oblong','Oval','Round', 'Square']\n",
    "fig_path = 'C:\\\\Users\\\\tprak_o7ept1f\\\\OneDrive\\\\Desktop\\\\Face_Shape_Classification\\\\images\\\\'\n",
    "\n",
    "def create_confusion_matrix(y_test_labels, y_preds, classes, title='Confusion Matrix', \n",
    "                            normalize=False, cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = confusion_matrix(y_test_labels, y_preds)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize = 15)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f\"{fig_path+title}.png\");    # for saving images to .png file\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_dict = {0: 'Heart', 1: 'Oblong', 2: 'Oval', 3: 'Round', 4: 'Square'}\n",
    "\n",
    "def show_X_img(num, index_array, X_array, y_actual, fig_title, ncols=1):\n",
    "    '''this function display images from an image array'''\n",
    "    ncols= ncols\n",
    "    nrows= int(num/ncols)\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize =(ncols*4,nrows*4))\n",
    "    fig.suptitle(fig_title, size = 20)\n",
    "    ax = ax.ravel()\n",
    "    for i, index in enumerate(index_array):\n",
    "        img = X_array[index] * 255\n",
    "        img = np.asarray(img, int)\n",
    "        label = y_label_dict[y_actual[index]]\n",
    "        ax[i].imshow(img, cmap='gray')\n",
    "        ax[i].set_title(label, size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_misclass(df, predict, fig_title='Comparing Misclassification', ax_title=''):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,5))\n",
    "    fig.suptitle(fig_title, fontsize=16)\n",
    "    ax[0].hist(df.y_actual, bins=9, color='lightcoral')\n",
    "    ax[0].set_xticks(range(0,5))\n",
    "    ax[0].set_xticklabels(cm_plot_labels)\n",
    "    ax[0].set_title(f'{ax_title}\\n ACTUAL CLASS')\n",
    "    ax[1].hist(df[predict], bins=9, color='mediumturquoise')\n",
    "    ax[1].set_xticks(range(0,5))\n",
    "    ax[1].set_xticklabels(cm_plot_labels)\n",
    "    ax[1].set_title(f'{ax_title}\\n PREDICTED CLASS')\n",
    "    plt.savefig(f\"{fig_path+fig_title}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_misclass_img_proba(df, list_index):\n",
    "  for i in list_index:\n",
    "    img = X_test[i] * 255\n",
    "    img = np.asarray(img, int)\n",
    "    label = y_label_dict[y_actual[i]]\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(label, size=16)\n",
    "    plt.subplot(1,3,2)\n",
    "    df.loc[i, ['heart_s', 'oblong_s', 'oval_s', 'round_s','square_s']].plot(kind='bar', color='pink')\n",
    "    plt.title('Probabilities - CNN from scratch', fontsize=14, y=1.01)\n",
    "    plt.subplot(1,3,3)\n",
    "    df.loc[i, ['heart_t', 'oblong_t', 'oval_t', 'round_t','square_t']].plot(kind='bar', color='plum')\n",
    "    plt.title('Probabilities - CNN transfer learning', fontsize=14, y=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\tprak_o7ept1f\\\\OneDrive\\\\Desktop\\\\Face_Shape_Classification\\\\\"\n",
    "\n",
    "X_train = np.asarray(pickle.load(open(path + \"dataX_train_rgb.pickle\",\"rb\")))\n",
    "y_train = np.asarray(pickle.load(open(path + \"datay_train_rgb.pickle\",\"rb\")))\n",
    "X_test = np.asarray(pickle.load(open(path + \"dataX_test_rgb.pickle\",\"rb\")))\n",
    "y_test = np.asarray(pickle.load(open(path + \"datay_test_rgb.pickle\",\"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Summary\n",
      "--------------------\n",
      "X_train shape (4000, 224, 224, 3)\n",
      "y_train shape (4000, 5)\n",
      "--------------------\n",
      "X_test shape (1000, 224, 224, 3)\n",
      "y_test shape (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Summary\")\n",
    "print(\"--------------------\")\n",
    "print(f\"X_train shape {X_train.shape}\")\n",
    "print(f\"y_train shape {y_train.shape}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"X_test shape {X_test.shape}\")\n",
    "print(f\"y_test shape {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "transfer_path = \"C:\\\\Users\\\\tprak_o7ept1f\\\\OneDrive\\\\Desktop\\\\Face_Shape_Classification\\\\saved_models\\\\\"\n",
    "transfer_file = transfer_path + 'vgg16-face-2.h5'\n",
    "transfer_file1 = transfer_path + 'vgg16-face-1.h5'\n",
    "mod_transfer = tf.keras.models.load_model(transfer_file)\n",
    "mod_transfer1 = tf.keras.models.load_model(transfer_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 3s/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_actual = np.argmax(y_test, axis=-1)\n",
    "y_predict_scratch = np.argmax(mod_transfer1.predict(X_test), axis=1)\n",
    "y_predict_transfer = np.argmax(mod_transfer.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 3s/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 3s/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predict probabilities\n",
    "predict_proba_scratch = mod_transfer1.predict(X_test)\n",
    "predict_proba_transfer = mod_transfer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 3s/step - accuracy: 0.9126 - loss: 0.4304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3667786121368408, 0.9120000004768372]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_transfer1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 3s/step - accuracy: 0.8964 - loss: 0.5279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4422782063484192, 0.9079999923706055]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_transfer.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(y_actual, y_predict, predict_proba, X_test, y_test):\n",
    "  '''this function creates a dataframe with predictions and probabilities of a model'''\n",
    "  # generate predictions\n",
    "\n",
    "  actual = pd.DataFrame(y_actual, columns=['y_actual'])\n",
    "  predict = pd.DataFrame(y_predict, columns=['y_predict'])\n",
    "\n",
    "  # generate prediction probabilities \n",
    "\n",
    "  probability_list = []\n",
    "  for i, item in enumerate(predict_proba):\n",
    "    probabilities = {}\n",
    "    probabilities['heart'] = round(item[0] * 100,2)\n",
    "    probabilities['oblong'] = round(item[1] * 100,2)\n",
    "    probabilities['oval'] = round(item[2] *100,2)\n",
    "    probabilities['round'] = round(item[3] *100,2)\n",
    "    probabilities['square'] = round(item[4] *100,2)\n",
    "    probability_list.append(probabilities)\n",
    "  proba = pd.DataFrame(probability_list)\n",
    "\n",
    "  # create dataframe\n",
    "  df = pd.concat([actual, predict, proba],axis=1)\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer1 = create_df(y_actual, y_predict_scratch, predict_proba_scratch, X_test, y_test)\n",
    "\n",
    "transfer = create_df(y_actual, y_predict_transfer, predict_proba_transfer, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_actual_t</th>\n",
       "      <th>y_predict_t</th>\n",
       "      <th>heart_t</th>\n",
       "      <th>oblong_t</th>\n",
       "      <th>oval_t</th>\n",
       "      <th>round_t</th>\n",
       "      <th>square_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.77</td>\n",
       "      <td>98.120003</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>99.540001</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_actual_t  y_predict_t  heart_t  oblong_t  oval_t     round_t  square_t\n",
       "0           2            3     0.00       0.0    1.77   98.120003      0.12\n",
       "1           0            2     9.50       0.0   90.50    0.000000      0.00\n",
       "2           3            3     0.00       0.0    0.00  100.000000      0.00\n",
       "3           3            3     0.02       0.0    0.00   99.540001      0.44\n",
       "4           0            0   100.00       0.0    0.00    0.000000      0.00"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe for model with transfer learning\n",
    "transfer.columns = [x + '_t' for x in transfer.columns]\n",
    "transfer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scratch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# combine the data frame for evaluations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m evaluations \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mscratch\u001b[49m, transfer], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# drop one of the y_actual as they are the same\u001b[39;00m\n\u001b[0;32m      5\u001b[0m evaluations \u001b[38;5;241m=\u001b[39m evaluations\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_actual_t\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scratch' is not defined"
     ]
    }
   ],
   "source": [
    "# combine the data frame for evaluations\n",
    "evaluations = pd.concat([scratch, transfer], axis=1)\n",
    "\n",
    "# drop one of the y_actual as they are the same\n",
    "evaluations = evaluations.drop(columns='y_actual_t')\n",
    "evaluations = evaluations.rename(columns={'y_actual_s': 'y_actual'})\n",
    "\n",
    "# create columns with actual labels\n",
    "evaluations['actual'] = evaluations['y_actual'].map(y_label_dict)\n",
    "evaluations['s_predict'] = evaluations['y_predict_s'].map(y_label_dict)\n",
    "evaluations['t_predict'] = evaluations['y_predict_t'].map(y_label_dict)\n",
    "\n",
    "# create new columns to detect where the 2 models misclassify, and the differences in predictions\n",
    "evaluations['predict_diff'] = evaluations['y_predict_s'] - evaluations['y_predict_t']\n",
    "evaluations['t_misclass'] = evaluations['y_actual'] - evaluations['y_predict_t']\n",
    "evaluations['s_misclass'] = evaluations['y_actual'] - evaluations['y_predict_s']\n",
    "\n",
    "evaluations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
